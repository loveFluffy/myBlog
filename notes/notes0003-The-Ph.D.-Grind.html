<!DOCTYPE html>

<html lang="en" xmlns="http://www.w3.org/1999/xhtml">
<head>
    <meta charset="utf-8" />
    <link rel="stylesheet" href="../stylesheets/notestyles.css" />
    <title>The Ph.D. Grind</title>
    <link rel="icon" href="../images/x.ico" />
</head>
<body>
    <h1>Notes#0003: The Ph.D. Grind</h1>
    <h2><a href="http://www.pgbovine.net/index.html">Philip J. Guo</a></h2>



    <!-- ======================== Year One: Downfall ======================== -->
    <h3>Year One: Downfall</h3>
    <div>
        <p>When I arrived on campus, Dawson was a recently-tenured professor who had been at Stanford for the past eight years; professors usually earn tenure (a lifetime employment guarantee) if they have published enough notable papers in their first seven years on the job. Dawson's main research interest was in building innovative tools that could automatically find bugs (errors in software code) in complexpieces of real-world software. Over the past decade, Dawson and his students built several tools that were able to find far more bugs than any of their competitors. Their research techniques were so effective that they created a successful startup company to sell software bug-finding services based on those techniques. Although I somewhat liked Dawson's projects, what appealed more to me was that his research philosophy matched my own: He was an ardent pragmatist who cared more about achieving compelling results than demonstrating theoretical "interestingness" for the sake of appearing scholarly.</p>
        <p>During my first meeting with Dawson, he seemed vaguely interested in my broader goals of making computer usage and programmingmore productive. However, he made it very clear that he wanted to recruit new students to work on an automatic bug-finding tool called Klee that his grant money was currently funding. (The tool has had several names, but I will call it "Klee" for simplicity.) From talking with other professors and senior Ph.D. students in my department, I realized it was the norm for new students to join an existing grantfunded research project rather than to try creating their own original project right away. I convinced myself that automatically finding software bugs was an indirect way to make programmers more productive, so I decided to join the Klee project.</p>

        <h4 align="center">~</h4>
        <p>After Dawson made it clear that he wanted to aim for that particular March 2007 top-tier conference submission deadline, he told me what the other five students were currently working on and gave options for tasks that I could attempt. I chose to use Klee to find new bugs in Linux device drivers. A device driver is a piece of software code that allows the operating system to communicate with a hardware peripheral such as a mouse or keyboard. The Linux operating system (similar to Microsoft Windows or Apple Mac OS) contains thousands of device drivers to connect it with many different kinds of hardware peripherals. Bugs in device driver code are both hard to find using traditional means and also potentially dangerous, because they can cause the operating system to freeze up or crash.</p>
        <p>Dawson believed that Klee could find new bugs that no automated tool or human being had previously found within the code of thousands of Linux device drivers. I remember thinking that although new Linux device driver bugs could be cool to present in a paper, it wasn't clear to me how these results constituted a real research contribution. From my understanding, I was going to use Klee to find new bugs— an application of existing research—rather than improving Klee in an innovative way. Furthermore, I couldn't see how my project would fit together with the other five students' projects into one coherent paper submission in March. However, I trusted that Dawson had the highlevel paper writing strategy in his head. I had just joined the project, so I didn't want to immediately question these sorts of professor-level decisions. I was given a specific task, so I wanted to accomplish it to the best of my abilities.</p>
    </div>



    <!-- ======================== Year Two: Inception ======================== -->
    <h3>Year Two: Inception</h3>
    <div>
        <p>Right before starting my second year of Ph.D. in September 2007, I took a one-week vacation to Boston to visit college friends. Since I was in the area, I emailed a few MIT professors whom I knew from my undergraduate days to ask for their guidance. When they met with me, they all told me roughly the same thing: Be proactive in talking with professors to find research topics that are mutually interesting, and no matter what, don't just hole up in isolation. This simple piece of advice, repeatedly applied over the next five years, would ultimately lead me to complete my Ph.D. on a happy note.</p>
        <p>When I returned to Stanford, I cold-emailed Scott to set up an appointment to chat. I came into the meeting prepared with notes about three specific ideas and pitched them in the following format:</p>
        <ul>
            <li>1. What's the problem?</li>
            <li>2. What's my proposed solution?</li>
            <li>3. What compelling experiments can I run to demonstrate the effectiveness of my solution?</li>
        </ul>

        <h4 align="center">~</h4>
        <p>Joel continued down this research path by turning our paper into the first contribution of his dissertation. Over the next few years, he built several tools to help programmers overcome some of the problems we identified in that initial study and published papers describing those tools. In the meantime, Scott didn't actively recruit me to become his student, so I never thought seriously about switching advisors. It seemed like my interests were too similar to Joel's, and Joel was already carving out a solid niche for himself in his subfield. Thus, I focused my efforts on my main project with Dawson, since he was still my advisor. Progress was not as smooth on that front, though.</p>

        <h4 align="center">~</h4>
        <p>Recall that at the end of my first year, I started exploring research ideas in a subfield called empirical software measurement—specifically, trying to measure software quality by analyzing the development history of software projects. It turned out that Dawson was also interested in this topic, so we continued working together on it throughout my second year. His main interest was in building new automated bug-finding tools (e.g., Klee), but he had a side interest in software quality in general. He was motivated by research questions such as:</p>
        <ul>
            <li>If a large software project has, say, 10 million lines of code, which portions of that code are crucial to the project, and which are not as important?</li>
            <li>What factors affect whether a section of code is more likely to contain bugs? For example, does code that has been recently modified by novices contain more bugs? What about code that has been modified by many people over a short span of time?</li>
            <li>f an automated bug-finding tool finds, say, 1,000 possible bugs, which ones are likely to be important? Programmers have neither the time nor energy to triage all 1,000 bug reports, so they must prioritize accordingly.</li>
        </ul>

        <h4 align="center">~</h4>
        <p>Dawson and I had a lot of trouble getting our results published. Throughout the year, we made two paper submissions that were both rejected. It would take another full year before this work finally got published as a shorter-length, second-tier conference paper, which held almost no prestige and didn't "count" as a contribution to my dissertation. But by then, I didn't care because I had already moved on to other projects.</p>
        <p>Since conferences usually accept less than 20 percent of paper submissions, if reviewers get a bad first impression when reading a paper, then they are likely to reject it. Dawson and I were not specialists in the empirical software measurement subfield, so we weren't able to "pitch" our paper submissions in a way that appealed to reviewers' expectations. Thus, we repeatedly got demoralized by negative reviews such as, "In the end, I find I just don't have much confidence in the results the authors present. There are two sources of doubt: I don't trust their interpretation of the measures, and they don't use very effective statistical techniques." In the cutthroat world of academic publishing, simply being passionate about a topic is nowhere near sucient for success; one must be well-versed in the preferences of senior colleagues in a particular subfield who are serving as paper reviewers. In short, our data sets were not as good, our techniques were not as refined, and our results and presentation style were less impressive than what the veterans in this subfield expected.</p>
        <p>By the end of my second year of Ph.D. (June 2008), I was growing frustrated by my lack of compelling results and overwhelmed by the flurry of papers being published in the empirical software measurement subfield. I still hadn't been able to publish my own findings and realized that I couldn't effectively compete with the veterans: Since neither Dawson nor I knew what those paper reviewers wanted to see, I sensed that trying to publish in this subfield would be a continual uphill struggle. And since publishing was a prerequisite for graduation, I had to find another project as soon as possible, or else I wouldn't be able to earn a Ph.D. As I prepared to begin my third year at Stanford, I was desperate to cling onto anything that had a reasonable chance of producing publishable results. And that's when I returned back to the project that haunted me from my first year—Klee.</p>
    </div>



    <!-- ======================== Year Three: Relapse ======================== -->
    <h3>Year Three: Relapse</h3>
    <div>
        <p>So what was different here? In short, neither Cristi nor Dawson were truly hungry to publish. They had already published several Klee papers together, and a cross-checking paper coauthored with me would have been a "nice-to-have" but not mandatory follow-up publication. Cristi was in his final year of Ph.D. and didn't need to publish any more papers to graduate, and Dawson already had tenure, so he wasn't in a rush to publish either. In contrast, Joel was a mid-stage Ph.D. student who was itching to publish the first paper of his dissertation, and Scott was an assistant professor who needed to publish prolifically to earn tenure. These two opposing experiences taught me the importance of deeply understanding the motivations and incentives of one's potential collaborators before working with them.</p>

        <h4 align="center">~</h4>
        <p>The professor might need to go through several rounds of student failures and dropouts before one set of students eventually succeeds. Sometimes that might take two years, sometimes five years, or sometimes even ten years to achieve. Many projects last longer than individual Ph.D. student "lifetimes." But as long as the original vision is realized and published, then the project is considered a success. The professor is happy, the university department is happy, the grant funding agency is happy, and the final surviving set of students is happy. But what about the student casualties along the way? A tenured professor can survive several years' worth of failures, but a Ph.D. student's fledgling career—and psychological health—will likely be ruined by such a chain of disappointments.</p>

        <h4 align="center">~</h4>
        <p>I calculated that the only advantage of staying with Klee was that Dawson deeply loved the project. Even if I couldn't get any papers published, he could maybe appeal to let me "pity graduate" with zero publications. But given my painful past with Klee, I couldn't stomach the possibility of grinding on it for an unknown number of additional years just for the hope of a pathetic "pity graduation."</p>
        <p> By now, I had finished three years of my Ph.D. still without any idea of how I was going to eventually put together a dissertation. I had no concrete plan looking forward, but I knew that I wanted to get away from Klee once and for all.</p>
    </div>



    <!-- ======================== Intermission ======================== -->
    <h3>Intermission</h3>
    <div>
        <p>And then, on July 24, 2009—halfway through my internship— inspiration suddenly struck. In the midst of writing computer programs in my MSR oce to process and analyze data, I came up with the initial spark of an idea that would eventually turn into the first project of my dissertation. I frantically scribbled down pages upon pages of notes and then called my friend Robert to make sure my thoughts weren't totally ludicrous. At the time, I had no sense of whether this idea was going to be taken seriously by the wider academic community, but at least I now had a clear direction to pursue when I returned to Stanford to begin my fourth year.</p>
    </div>



    <!-- ======================== Year Four: Reboot ======================== -->
    <h3>Year Four: Reboot</h3>
    <div>
        <p>Throughout my second year of Ph.D. and my summer 2009 internship at MSR, I performed empirical software measurement research by writing computer programs in a popular language called Python to process, analyze, and visualize data sets. After writing hundreds of these sorts of Python programs, I noticed that I kept facing similar ineciencies over and over again. In particular, I had to tediously keep track of a multitude of data files on my computer and which programs they depended upon; I also needed to make my programs unnecessarily complex so that they could execute (run) quickly after each round of incremental changes to my analyses. After letting these observations simmer in the back of my mind during the summer, the following idea suddenly came to me on that quiet July afternoon at MSR: By altering the Python run-time environment (called an interpreter ) in some innovative ways, I could eliminate many of these ineciencies, thereby improving the productivity of computational researchers who use Python. I named my proposed modification to the Python interpreter "IncPy," which stands for Incremental Python.</p>

        <h4 align="center">~</h4>
        <p>The argument I wanted to make was that lots of computational researchers in diverse fields struggle with several common ineciencies in their daily programming workflow, and IncPy provides a new kind of fully automated solution to such ineciencies that nobody else has previously implemented. This initial pitch would later evolve into the theme of my entire dissertation.</p>

        <h4 align="center">~</h4>
        <p>Even at the start of my IncPy project, I knew that it would be difficult to present a convincing evaluation because the argument I wanted to make—that IncPy can improve the productivity of computational researchers—was a subjective and fuzzy notion. After reading several other papers that presented similar productivity improvement claims, I devised a two-part evaluation strategy:</p>
        <ul>
            <li>1. Case Studies: Get a collection of programs written in the Python language from a variety of computational researchers and then simulate the productivity improvements they would have achieved if they had used IncPy during their research rather than regular Python.</li>
            <li>2. Deployment: Get some researchers to use IncPy rather than regular Python in their daily work and have them report if and how IncPy improved their productivity.</li>
        </ul>

        <h4 align="center">~</h4>
        <p>By the time I tried installing IncPy on the Berkeley neuroscientists' computers, I had been building and testing it for seven months, so I felt reasonably confident that it would work for them. However, shortly after installation, we discovered that IncPy was not compatible with dozens of third-party Python add-ons (called extension modules) that these scientists were using in their daily work. In my own private testing, I tested only the basic use cases without any extension modules. I was hit with a lesson in the harshness of real-world deployment: failures come in unexpected forms, and once the user gets a bad first impression, then it's over! Like most researchers creating prototypes in an ivory tower, I could have never predicted that this unforeseen banal extension module issue would completely derail my first deployment attempt.</p>
    </div>



    <!-- ======================== Year Five: Production ======================== -->
    <h3>Year Five: Production</h3>
    <div>
        <p>On July 29, 2010, almost exactly one year after I conceived the initial idea for IncPy, I came up with a related idea, again inspired by real problems that computational researchers encounter while performing data analysis. I observed that because researchers write computer programs in an ad-hoc "sloppy" style, their programs often crash for silly reasons without producing any analysis results, thus leading to table-pounding frustration. My insight was that by altering the runtime environment (interpreter ) of the Python programming language, I could eliminate all of those crashes and allow their sloppy programs to produce partial results rather than no results. I named this proposed modification to the Python interpreter "SlopPy," which stands for Sloppy Python.</p>

        <h4 align="center">~</h4>
        <p>Specifically, I noticed that researchers edit and execute their Python programs dozens to hundreds of times per day while running computational experiments; they repeat this process for weeks or months at a time before making a significant discovery. I had a hunch that recording and comparing what changed between program executions could be useful for debugging problems and obtaining insights. To facilitate such comparisons, I planned to extend IncPy to record details about which pieces of code and data were accessed each time a Python program executes, thereby maintaining a rich history of a computational experiment. I also thought it would be cool for researchers to share these experiment histories with colleagues so that they can learn from what worked and didn't work during experimental trials.</p>

        <h4 align="center">~</h4>
        <p>As soon as I mentioned my ideas about extending IncPy to record Python-based experiment histories, Fernando launched into a passionate sermon about a topic that I had never heard of but was fascinated by: reproducible research.</p>
        <p>Why is reproducibility so hard to achieve in practice? A few ultra-competitive scientists purposely hide their computer code and data to fend off potential rivals, but the majority are willing to share code and data upon request. The main technical barrier, though, is that simply obtaining someone's code and data isn't enough to rerun and reproduce their experiments. Everyone's code needs a highlyspecific environment in which to run, and the environments on any two computers—even those with the same operating system—differ in subtle and incompatible ways. Thus, if you send your code and data to colleagues, they probably won't be able to rerun your experiments.</p>
        <p>As I was jotting down more detailed notes, a moment of extreme clarity struck: Why limit this experiment recording to only Python programs? With some generalizations to my idea, I could make a tool that enables the easy reproduction of computational experiments written in any programming language! Still in a mad frenzy, I sketched out the design for a new tool named "CDE," which stands for Code, Data, and Environment.</p>
    </div>



    <!-- ======================== Year Six: Endgame ======================== -->
    <h3>Year Six: Endgame</h3>
    <div>
        <p></p>
        <p></p>
        <p></p>
        <p></p>
    </div>



    <!-- ======================== Epilogue ======================== -->
    <h3>Epilogue</h3>
    <div>
        <p>If you are not going to become a professor, then why even bother pursuing a Ph.D.? This frequently-asked question is important because most Ph.D. graduates aren't able to get the same jobs as their university mentors and role models—tenure-track professors. There simply aren't enough available faculty positions, so most Ph.D. students are directly training for a job that they will never get. (Imagine how disconcerting it would be if medical or law school graduates couldn't get jobs as doctors or lawyers, respectively.)</p>
        <p>So why would anyone spend six or more years doing a Ph.D. when they aren't going to become professors? Everyone has different motivations, but one possible answer is that a Ph.D. program provides a safe environment for certain types of people to push themselves far beyond their mental limits and then emerge stronger as a result. For example, my six years of Ph.D. training have made me wiser, savvier, grittier, and more steely, focused, creative, eloquent, perceptive, and professionally effective than I was as a fresh college graduate. (Two obvious caveats: Not every Ph.D. student received these benefits—many grew jaded and burned-out from their struggles. Also, lots of people cultivate these positive traits without going through a Ph.D. program.)</p>

        <h4 align="center">~</h4>
        <p>Here are twenty of the most memorable lessons that I've learned throughout my Ph.D. years. My purpose in sharing is not to provide unsolicited advice to students, since everyone's Ph.D. experience differs greatly; nor is it to encourage people to pursue a Ph.D., since these lessons can come from many sources. Rather, this section merely serves as a summary of what I gained from working towards my Ph.D.</p>
        <p><strong>1. Results trump intentions: </strong>Nobody questions someone's intentions if they produce good results. I didn't have so-called pure intellectual motivations during grad school: I started a Ph.D. because I wasn't satisfied with engineering jobs, pressured myself to invent my own projects out of fear of not graduating on time, and helped out on HCI projects with Scott, Joel, and Jeff to hedge my bets. But I succeeded because I produced results: five prototype tools and a dozen published papers. Throughout this process, I developed strong passions for and pride in my own work. In contrast, I know students with the most idealistic of intentions—dreamy and passionate hopes of revolutionizing their field—who produce few
        results and then end up disillusioned.</p>
        <p><strong>2. Outputs trump inputs: </strong>The only way to earn a Ph.D. is by successfully producing research outputs (e.g., published papers), not merely by consuming inputs from taking classes or reading other people's papers. Of course, it's absolutely necessary to consume before one can produce, but it's all too easy to over-consume. I fell into this trap at the end of my first year when I read hundreds of research papers in a vacuum—a consumption binge—without being able to synthesize anything useful from my undirected readings. In contrast, related work literature searches for my dissertation projects were much more effective because my reading was tightly directed towards clear goals: identifying competitors and adapting good ideas into my own projects.</p>
        <p><strong>3. Find relevant information: </strong>My Ph.D. training has taught me how to effectively find the most relevant information for what I need to accomplish at each moment. Unlike traditional classroom learning, when I'm working on research, there are no textbooks, no lecture notes, and no instructors to provide definitive answers. Sometimes what I need for my work is in a research paper, sometimes it's within an ancient piece of computer code, sometimes it's on an obscure website, and sometimes it's inside the mind of someone whom I need to track down and ask for help.</p>
        <p><strong>4. Create lucky opportunities: </strong>I got incredibly lucky several times throughout grad school, culminating in getting to work with Margo at Harvard during my final year. But these fortuitous opportunities wouldn't have arisen if I didn't repeatedly put myself and my work on display—giving talks, chatting with colleagues, asking for and offering help, and expressing gratitude. The vast majority of my efforts didn't result in serendipity, but if I didn't keep trying, then I probably wouldn't have gotten lucky.</p>
        <p><strong>5. Play the game: </strong>As a Ph.D. student, I was at the bottom of the pecking order and in no position to change the "academic game." Specifically, although I dreaded getting my papers repeatedly rejected, I had no choice but to keep learning to play the publication game to the best of my abilities. However, I was happy that I played in my own unique and creative way during the second half of grad school by pursuing more unconventional projects while still conforming to the "rules" well enough to publish and graduate.</p>
        <p><strong>6. Lead from below: </strong>By understanding the motivations and personalities of older Ph.D. students, professors, and other senior colleagues, I was able to lead my own initiatives even from the bottom of the pecking order. For example, after I learned Margo's research tastes by reading her papers and grant applications, I came up with a project idea (Burrito) that we were both excited about. If I were oblivious to her interests, then it would have been much harder to
        generate ideas to her liking.</p>
        <p><strong>7. Professors are human: </strong>While this might sound obvious, it's all too easy to forget that professors aren't just relentless researchproducing machines. They're human beings with their own tastes, biases, interests, motivations, shortcomings, and fears. Even wellrespected science-minded intellectuals have subjective and irrational quirks. From a student's perspective, since professors are the gatekeepers to publication, graduation, and future jobs, it's important to empathize with them both as professionals and also as people.</p>
        <p><strong>8. Be well-liked: </strong>I was happier and more productive when workingvwith people who liked me. Of course, it's impossible to be wellliked by all colleagues due to inevitable personality differences. In general, I strived to seek out people with whom I naturally clicked well and then took the time to nurture those relationships.</p>
        <p><strong>9. Pay some dues: </strong>It's necessary for junior lab members to pay their dues and be "good soldiers" rather than making presumptuous demands from day one. As an undergraduate and master's student at MIT, I paid my dues by working on an advisor-approved, grantfunded project for two and a half years rather than trying to create my own project; I was well-rewarded with admissions into topranked Ph.D. programs and two fellowships, which paid for five years of graduate school. However, once I started at Stanford, I paid my dues for a bit too long on the Klee project before quitting. It took me years to recognize when to defer to authority figures and when to selfishly push forward my own agenda.</p>
        <p><strong>10. <big><u>Reject bad defaults</u></big>: </strong>Defaults aren't usually in the best interests of those on the bottom (e.g., Ph.D. students), so it's important to know when to reject them and to ask for something different. Of course, there's no nefarious conspiracy against students; the defaults are just naturally set up to benefit those in power. For example, famous tenured professors like Dawson are easily able to get multi-year grants to fund students to work on "default" projects like Klee. As long as some papers get published from time to time, then the professor and project are both viewed as successful, regardless of how many students stumbled and failed along the way. Students must judge for themselves whether their default projects are promising, and if not, figure out how to quit gracefully.</p>
        <p><strong>11. <big><u>Know when to quit</u></big>: </strong>Quitting Klee at the end of my third year was my most pivotal decision of grad school. If I hadn't quit Klee, then there would be no IncPy, no SlopPy, no CDE, no ProWrangler, and no Burrito; there would just be three or more years of painful incremental progress followed by a possible "pity graduation."</p>
        <p><strong>12. <big><u>Recover from failures</u></big>: </strong>Failure is inevitable in grad school. Nothing I did during my first three years made it into my dissertation, and many paths I wandered down in my latter three years were also dead-ends. Grad school was a safe environment to practice recovering from failures, since the stakes were low compared to failing in real jobs. In my early Ph.D. years, I would grow anxious, distraught, and paralyzed over research failures. But as I matured, I learned to channel my anger into purposeful action in what I call a productive rage. Every rejection, doubt, and criticism spurred me to work harder to prove the naysayers wrong. Lessons learned from earlier failures led to successes later in grad school. For example, my failure to shadow professional programmers at the beginning of my second year taught me how and who to approach for these sorts of favors, so I later succeeded at shadowing computational researchers to motivate my dissertation work; and my failure to get lots of real users for IncPy taught me how to better design and advertise my software so that I could get 10,000 users for CDE.</p>
        <p><strong>13. <big><u>Ally with insiders</u></big>: </strong>I had an easy time publishing papers when allied with expert insiders such as Scott and Joel during my second year, Tom during my MSR internship, and Jeff during my fifth year. They knew all the tricks of the trade required to get papers published in their respective subfields; the five papers that I co-wrote with these insiders were all accepted on their first submission attempts. However, struggling as an outsider—with Dawson on empirical software measurement in my second year and then on my solo dissertation projects—was also enriching, albeit more frustrating due to repeated paper rejections.</p>
        <p><strong>14. <big><u>Give many talks</u></big>: </strong>I gave over two dozen research presentations throughout my Ph.D. years, ranging from informal talks at university lab group meetings to conference presentations in large hotel ballrooms. The informal talks I gave at the beginning of projects such as IncPy were useful for getting design ideas and feedback; those I gave prior to submitting papers were useful for discovering common criticisms that I needed to address in my papers. Also, every talk was great practice for improving my skills in public speaking and in responding to sometimes-hostile questions. Finally, talks sometimes sparked follow-up discussions that led to serendipity: For example, after watching my first talk on IncPy, a fellow grad student emailed me a link to Fernando's blog post about Python in science; that email encouraged me to reach out to Fernando, who would later inspire me to improve IncPy and then to invent CDE. Over a year later, my Google Tech Talk on CDE directly led to my super-chill summer 2011 internship. </p>
        <p><strong>15. Sell, sell, sell: </strong>I spent the majority of my grad school days headsdown grinding on implementing research ideas, but I recognized that convincingly selling my work was the key to publication, recognition, and eventual graduation. Due to the ultra-competitive nature of the paper publication game, what often makes the difference between an accept and a reject decision is how well a paper's "marketing pitch" appeals to reviewers' tastes. Thus, thousands of hours of hard grinding would go to waste if I failed to properly pitch the big-picture significance of my research to my target audience: senior academic colleagues. More generally, many people in a field have good ideas, so the better salespeople are more likely to get their ideas accepted by the establishment. As a low-status grad student, one of the most effective ways for me to "sell" my ideas and projects was to get influential people (e.g., famous professors such as Margo) excited enough to promote them on my behalf.</p>
        <p><strong>16. Generously provide help: </strong>One of my favorite characteristics of the Ph.D. experience was that I wasn't in competition with my classmates; it wasn't like if they did better, then I would do worse, or vice versa. Therefore, many of us generously helped one another, most notably by giving feedback on ideas and paper drafts before they were subject to the harsher critiques of external reviewers.</p>
        <p><strong>17. <big><u>Ask for help</u></big>: </strong>Over the past six years, I became good at determining when, who, and how to ask for help. Specifically, whenever I felt stuck, I sought experts who could help me get unstuck. Finding help can be as simple as asking a friend in my department, or it might require getting referrals or even cold-emailing strangers.</p>
        <p><strong>18. <big><u>Express true gratitude</u></big>: </strong>I learned to express gratitude for the help that others have given me throughout the years. Even though earning a Ph.D. was a mostly-solitary process, I wouldn't have made it without the generosity of dozens of colleagues. People feel good when they find out that their advice or feedback led to concrete benefits, so I strive to acknowledge everyone's specific contributions whenever possible. Even a quick thank-you email goes a long way.</p>
        <p><strong>19. Ideas beget ideas: </strong>As I discovered at the end of my first year, it's nearly impossible to come up with substantive ideas in a vacuum. Ideas are always built upon other ideas, so it's important to find a solid starting point. For instance, the motivations for both IncPy and SlopPy came from my frustrations with programmingrelated ineciencies I faced during my 2009 MSR internship. A year later, some of my ideas for extending IncPy, mixed with Fernando's insights on reproducible research and Dawson's mention of Linux dependency hell, led to the creation of CDE. Also, ideas can sometimes take years to blossom, usually after several false starts: I started pondering Burrito-like ideas during my second year and then at the end of my fourth, but it wasn't until my sixth year that I was able to solidify those fuzzy thoughts into a real project.</p>
        <p><strong>20. Grind hard and smart: </strong>This book is named The Ph.D. Grind because there would be no Ph.D. without ten thousand hours of unglamorous, hard-nosed grinding. This journey has taught me that creative ideas mean nothing without the extreme effort to bring them to fruition: showing up to the oce, getting my butt in the seat, grinding hard to make small but consistent progress, taking breaks to reflect and refresh, then repeating day after day for over two thousand consecutive days. However, grinding smart is just as important as grinding hard. <big>It's sad to see students blindly working themselves to death on tasks that won't get favorable results: approaching a research problem from an unwise angle, using the wrong kinds of tools, or doing useless errands. Grinding smart requires perceptiveness, intuition, and a willingness to ask for help.</big></p>
    </div>


</body>
</html>